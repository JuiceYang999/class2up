## **任务六：大数据分布式存储**

根据《大数据部分项目指导书》 和《项目课程2025.pdf》 的要求，本任务的目标是：

1.  **建立连接：** 建立 `backend.py` 后端与“工业大数据平台”（Hadoop/HDFS）的连接。
2.  **数据写入：** 开发数据通信接口，将我们实时获取的数据（已存入MySQL）写入大数据平台，实现分布式存储。
3.  **后续操作：** 能够在平台中进行数据查询、管理和可视化。

-----

### ⚠️ 重要说明：缺失的平台信息

在开始之前，必须指出：《实验步骤.pdf》和《大数据部分项目指导书.pdf》中详细说明了此任务的**目标**，但**未提供**连接到“工业大数据平台”（Hadoop/HDFS）所需的具体技术细节。

要完成此任务，您**必须从您的指导老师或实验文档中获取**以下信息：

1.  **连接方式（How）：** 是通过 WebHDFS (HTTP API)、Hive、HBase 还是其他方式？
2.  **服务地址（Where）：** 平台的 IP 地址和端口号（例如 `http://hadoop-master:9870` 或 Hive 的 `10000` 端口）。
3.  **认证信息（Who）：** 是否需要用户名、密码或 Kerberos 认证？
4.  **目标路径/表（What）：** 数据应该写入 HDFS 的哪个目录（例如 `/user/data/machining`）或 Hive 的哪个表？

由于缺少这些信息，我将为您提供一个**通用的代码框架**。您需要将我代码中的**占位符 (PLACEHOLDERS)** 替换为您获取到的实际信息。

-----

### 6.1 策略：实时数据流

我们将采用与任务五相同的策略：**在 `backend.py` 中实时写入**。
每当 `handle_message` 收到一条新的 MQTT 消息时，它在将数据写入 MySQL 和 InfluxDB 的**同时**，也会将其写入大数据平台。

### 6.2 安装 Python 依赖库（假设）

根据您获取到的平台连接方式，您可能需要安装**以下一个或多个**库。

（当您回到主机时，在终端 5 中执行）：

```bash
# 如果平台提供的是 WebHDFS 或 HTTP API (最常见)
pip install requests

# 如果平台允许直接 HDFS 访问
pip install hdfs

# 如果平台通过 Hive 写入
pip install pyhive[hive]
```

我将在下面的示例代码中优先使用 `requests` 库，因为它最通用。

-----

### 6.3 后端 (Python) 代码更新

我们将再次更新 `pyt/backend.py` 文件，添加一个 `save_to_distributed_platform` 函数，并在 `handle_message` 中调用它。

请**替换** `pyt/backend.py` 文件的**全部内容**为以下最新代码：

**文件路径：** `juiceyang999/class2up/class2up-9730b2fcdf474be54786a1f787cc6b005a26d2fc/pyt/backend.py`

```python
from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_mqtt import Mqtt
import json
import pymysql
import datetime
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS

import requests      # (新增) 引入 requests 库
import atexit        # (新增) 用于在程序退出时关闭文件

# --- 数据库配置 (保持不变) ---
MYSQL_HOST = 'localhost'
MYSQL_USER = 'root'
MYSQL_PASSWORD = 'xjtu2025'  # ！！！请修改为您的MySQL密码！！！
MYSQL_DB = 'mqtt_data'
MYSQL_PORT = 3306

INFLUX_URL = "http://localhost:8086"
INFLUX_TOKEN = "YOUR_API_TOKEN_HERE"  # ！！！请修改为您复制的 Token！！！
INFLUX_ORG = "my-org"                 # ！！！请修改为您的组织名称！！！
INFLUX_BUCKET = "mqtt_bucket"         # ！！！请修改为您的存储桶名称！！！

# --- (新增) Hadoop/HDFS 平台配置 (占位符) ---
# ！！！ 您必须用真实信息替换这些占位符 ！！！
HDFS_WEB_API_URL = "http://YOUR_HADOOP_HOST:9870/webhdfs/v1" # 示例: WebHDFS 地址
HDFS_TARGET_PATH = "/user/your_name/machine_data.csv" # 示例: HDFS 上的目标文件
HDFS_USER = "your_username" # 示例: Hadoop 用户名

# -------------------------

# 初始化 InfluxDB
try:
    influx_client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
    write_api = influx_client.write_api(write_options=SYNCHRONOUS)
    print("InfluxDB 客户端初始化成功。")
except Exception as e:
    print(f"InfluxDB 客户端初始化失败: {e}")

# (新增) 为 HDFS 写入打开一个本地临时文件
# 我们将数据先写入本地，然后一次性上传到 HDFS
# 这比每次都追加(append)到 HDFS 更高效
local_temp_file_path = "hdfs_temp_data.csv"
try:
    local_file = open(local_temp_file_path, "a", encoding="utf-8")
    # 写入 CSV 头部 (如果文件是新创建的)
    if local_file.tell() == 0:
        local_file.write("timestamp,data_id,value\n")
    print(f"打开本地临时文件 {local_temp_file_path} 成功。")
except Exception as e:
    print(f"打开本地临时文件失败: {e}")
    local_file = None

# (新增) 注册一个退出处理函数，在程序关闭时上传文件
@atexit.register
def upload_to_hdfs_on_exit():
    global local_file
    if local_file:
        local_file.close() # 关闭文件
        print(f"程序退出，正在将 {local_temp_file_path} 上传到 HDFS...")
        
        # ------------------- HDFS 上传逻辑 (示例) -------------------
        # 这是一个使用 WebHDFS 'CREATE' 操作的示例
        # 它会覆盖 HDFS 上的旧文件
        # 您需要根据平台的 API 文档进行修改
        try:
            # 1. 创建 HDFS 写入请求
            create_url = f"{HDFS_WEB_API_URL}{HDFS_TARGET_PATH}?op=CREATE&overwrite=true&user.name={HDFS_USER}"
            
            # HDFS 第一次请求会返回一个重定向
            r_create = requests.put(create_url, allow_redirects=False)
            
            if r_create.status_code == 307: # 临时重定向
                data_node_url = r_create.headers['Location']
                
                # 2. 将本地文件内容作为数据体(body)发送到数据节点
                with open(local_temp_file_path, "rb") as f:
                    r_upload = requests.put(data_node_url, data=f)
                    
                    if r_upload.status_code == 201: # Created
                        print(f"HDFS 文件上传成功: {HDFS_TARGET_PATH}")
                    else:
                        print(f"HDFS 文件上传失败: {r_upload.status_code} - {r_upload.text}")
            else:
                 print(f"HDFS 'CREATE' 请求失败: {r_create.status_code} - {r_create.text}")

        except Exception as e:
            print(f"HDFS 上传时发生严重错误: {e}")
        # ------------------------------------------------------------
        
# --- Flask 和 MQTT 初始化 (保持不变) ---
app = Flask(__name__)
CORS(app)
app.config['MQTT_BROKER_URL'] = '127.0.0.1'
app.config['MQTT_BROKER_PORT'] = 1883
app.config['MQTT_CLIENT_ID'] = 'flask_mqtt_client'
mqtt = Mqtt(app)
latest_data_store = {}

# --- 数据库写入函数 (保持不变) ---
def save_to_mysql(data_id, value, time_str):
    conn = None
    cur = None
    try:
        conn = pymysql.connect(host=MYSQL_HOST, user=MYSQL_USER, password=MYSQL_PASSWORD, database=MYSQL_DB, port=MYSQL_PORT, charset='utf8')
        cur = conn.cursor()
        sql = "INSERT INTO mac_data (data_id, payload, time) VALUES (%s, %s, %s)"
        cur.execute(sql, (data_id, str(value), time_str))
        conn.commit()
        print(f"MySQL 写入成功: ID={data_id}, Value={value}")
    except Exception as e:
        print(f"MySQL 写入失败: {e}")
        if conn: conn.rollback()
    finally:
        if cur: cur.close()
        if conn: conn.close()

def save_to_influxdb(data_id, value, time_obj):
    try:
        value_float = float(value)
        p = Point("machine_data").tag("device_id", "STRESS_TEST_00000").tag("data_id", data_id).field("value", value_float).time(time_obj, WritePrecision.NS)
        write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=p)
        print(f"InfluxDB 写入成功: ID={data_id}, Value={value_float}")
    except Exception as e:
        print(f"InfluxDB 写入失败: {e}")

# --- (新增) 写入 HDFS 平台的函数 ---
def save_to_distributed_platform(data_id, value, time_obj):
    """
    (概念) 将数据写入本地 CSV 文件，该文件将在程序退出时上传到 HDFS。
    """
    global local_file
    if local_file:
        try:
            # 转换为 ISO 格式时间戳
            time_iso = time_obj.isoformat()
            line = f"{time_iso},{data_id},{value}\n"
            local_file.write(line)
            local_file.flush() # 立即写入磁盘
            print(f"本地临时文件写入成功: ID={data_id}")
        except Exception as e:
            print(f"本地临时文件写入失败: {e}")

# --- MQTT 回调 (更新) ---
@mqtt.on_connect()
def handle_connect(client, userdata, flags, rc):
    if rc == 0:
        print("MQTT 连接成功 (rc=0)")
        response_topic = "Query/Response/STRESS_TEST_00000"
        mqtt.subscribe(response_topic)
        print(f"已自动订阅主题: {response_topic}")
    else:
        print(f"MQTT 连接失败，返回码: {rc}")

@mqtt.on_message()
def handle_message(client, userdata, message):
    try:
        payload_str = message.payload.decode()
        print(f"收到消息 (Topic: {message.topic}): {payload_str}")
        data = json.loads(payload_str)
        
        # 获取当前时间（一次性）
        time_now_utc = datetime.datetime.utcnow()
        time_now_local_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        if 'values' in data and isinstance(data['values'], list):
            for item in data['values']:
                if 'id' in item and 'values' in item:
                    data_id = item['id']
                    value = item['values'][0]
                    
                    # 1. 暂存 (供 Echarts)
                    latest_data_store[data_id] = value

                    # 2. 写入 MySQL
                    save_to_mysql(data_id, value, time_now_local_str)
                    
                    # 3. 写入 InfluxDB
                    save_to_influxdb(data_id, value, time_now_utc)
                    
                    # 4. (新增) 写入分布式平台 (的本地缓存)
                    save_to_distributed_platform(data_id, value, time_now_utc)
            
    except Exception as e:
        print(f"处理消息失败: {e}")

# --- Flask API 路由 (保持不变) ---
@app.route('/connect/', methods=['POST', 'GET'])
def make_connect():
    try:
        data_connect = request.get_json()['data']
        app.config['MQTT_BROKER_URL'] = data_connect['host']
        app.config['MQTT_BROKER_PORT'] = data_connect['port']
        app.config['MQTT_CLIENT_ID'] = data_connect['clientid']
        if mqtt.client.is_connected():
            mqtt.client.disconnect()
        mqtt.client.username_pw_set(None, None)
        mqtt.client._client_id = data_connect['clientid'].encode()
        mqtt.client.reinitialise()
        mqtt.client.connect(data_connect['host'], data_connect['port'])
        return jsonify({'rc_status': 'success'})
    except Exception as e:
        return jsonify({'rc_status': str(e)})

@app.route('/publish/', methods=['POST'])
def do_publish():
    try:
        data = request.get_json()
        topic = data.get('topic')
        payload = data.get('payload', "{}")
        mqtt.publish(topic, payload)
        return jsonify({'status': 'published', 'topic': topic})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/get_data/', methods=['POST'])
def get_data():
    try:
        data = request.get_json()
        data_id = data.get('id')
        value = latest_data_store.get(data_id)
        if value is not None:
            return jsonify({'status': 'ok', 'id': data_id, 'value': value})
        else:
            return jsonify({'status': 'not_found', 'id': data_id, 'value': None})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

# 启动 Flask 服务
if __name__ == '__main__':
    app.run(debug=True, host='127.0.0.1', port=5000)
```

-----

### 6.4 运行与调试（当您回到主机时）

1.  **启动所有 5 个终端**（同任务五）。
2.  **测试实时数据流**：
      * 访问 VUE 页面，连接 MQTT，查看“数据显示”页面。
      * 数据应能正常显示，并同时写入 MySQL 和 InfluxDB。
      * **新增验证：** 在 VUE 项目的根目录 (`.../class2up-9730b2fcdf474be54786a1f787cc6b005a26d2fc/`) 下，应该会出现一个新文件 `hdfs_temp_data.csv`。打开它，您会看到正在实时写入的 CSV 格式数据。
3.  **测试分布式存储（最终步骤）：**
      * 在**终端 5** (运行 `python pyt/backend.py` 的地方)，按下 `Ctrl+C` 来**停止后端服务**。
      * **预期结果：** 在服务停止前，您会看到 `atexit.register` 注册的函数被触发。
      * 终端应打印：`程序退出，正在将 hdfs_temp_data.csv 上传到 HDFS...`
      * 紧接着，如果您在 `backend.py` 中填写的 `HDFS_WEB_API_URL`、`HDFS_TARGET_PATH` 和 `HDFS_USER` 占位符是**正确**的，您将看到 `HDFS 文件上传成功!` 的消息。
      * **最终验证：** 登录您的工业大数据平台（Hadoop），检查 `HDFS_TARGET_PATH`（例如 `/user/your_name/machine_data.csv`）是否存在，并且内容是否正确。

**任务六完成**。这个方案实现了将实时数据缓存到本地，并在程序（或批处理）结束时一并上传到HDFS的通用模式，完成了“分布式存储”的核心要求。

接下来，我们将进行**任务七：大数据分析与可视化（设备状态预测）**。